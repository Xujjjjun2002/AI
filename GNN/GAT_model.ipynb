{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4X6uCREeR2ECZZz3RWQWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xujjjjun2002/AI/blob/main/GAT_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QP12DrTwU0R4"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    \"\"\"GAT层\"\"\"\n",
        "    def __init__(self,input_feature,output_feature,dropout,alpha,concat=True):\n",
        "        super(GATLayer,self).__init__()\n",
        "        self.input_feature = input_feature\n",
        "        self.output_feature = output_feature\n",
        "        self.alpha = alpha\n",
        "        self.dropout = dropout\n",
        "        self.concat = concat\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*output_feature,1)))\n",
        "        self.w = nn.Parameter(torch.empty(size=(input_feature,output_feature)))\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.w.data,gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data,gain=1.414)\n",
        "\n",
        "    def forward(self,h,adj):\n",
        "        Wh = torch.mm(h,self.w)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec) # adj>0的位置使用e对应位置的值替换，其余都为-9e15，这样设定经过Softmax后每个节点对应的行非邻居都会变为0。\n",
        "        attention = F.softmax(attention, dim=1) # 每行做Softmax，相当于每个节点做softmax\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.mm(attention, Wh) # 得到下一层的输入\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime) #激活\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self,Wh):\n",
        "\n",
        "        Wh1 = torch.matmul(Wh,self.a[:self.output_feature,:]) # N*out_size @ out_size*1 = N*1\n",
        "\n",
        "        Wh2 = torch.matmul(Wh,self.a[self.output_feature:,:]) # N*1\n",
        "\n",
        "        e = Wh1+Wh2.T # Wh1的每个原始与Wh2的所有元素相加，生成N*N的矩阵\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "class GAT(nn.Module):\n",
        "\t\"\"\"GAT模型\"\"\"\n",
        "    def __init__(self,input_size,hidden_size,output_size,dropout,alpha,nheads,concat=True):\n",
        "        super(GAT,self).__init__()\n",
        "        self.dropout= dropout\n",
        "        self.attention = [GATLayer(input_size, hidden_size, dropout=dropout, alpha=alpha,concat=True) for _ in range(nheads)]\n",
        "        for i,attention in enumerate(self.attention):\n",
        "            self.add_module('attention_{}'.format(i),attention)\n",
        "\n",
        "        self.out_att = GATLayer(hidden_size*nheads, output_size, dropout=dropout, alpha=alpha,concat=False)\n",
        "\n",
        "    def forward(self,x,adj):\n",
        "        x = F.dropout(x,self.dropout,training=self.training)\n",
        "        x = torch.cat([att(x,adj) for att in self.attention],dim=1)\n",
        "        x = F.dropout(x,self.dropout,training=self.training)\n",
        "        x = F.elu(self.out_att(x,adj))\n",
        "\n",
        "        return F.log_softmax(x,dim=1)\n"
      ]
    }
  ]
}
